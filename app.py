import streamlit as st
from ultralytics import YOLO
import cv2
import tempfile
import pickle
import numpy as np

# --- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ---
# –ó–∞–≥—Ä—É–∂–∞–µ–º YOLO –æ–¥–∏–Ω —Ä–∞–∑ –∏ —Ö—Ä–∞–Ω–∏–º –≤ –∫—ç—à–µ
@st.cache_resource
def load_yolo_model():
    model = YOLO('yolov8n-pose.pt')
    return model

# –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—à –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–¥–∏–Ω —Ä–∞–∑ –∏ —Ö—Ä–∞–Ω–∏–º –≤ –∫—ç—à–µ
@st.cache_resource
def load_classifier_model():
    with open('pose_classifier.pkl', 'rb') as f:
        model = pickle.load(f)
    return model

# --- –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---

st.set_page_config(layout="wide", page_title="Namaz Guide AI")

st.title("Namaz Guide AI ü§ñ")
st.write("---")
st.subheader("–í–∞—à –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–º–∞–∑–∞")
st.write("""
–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–æ–∑ –≤ –Ω–∞–º–∞–∑–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Ä–∞–∫–∞–∞—Ç–æ–≤. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞.
""")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
pose_model = load_yolo_model()
pose_classifier = load_classifier_model()

# –°–æ–∑–¥–∞–µ–º –≤–∏–¥–∂–µ—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª (mp4, mov, avi)", type=["mp4", "mov", "avi"])

if uploaded_file is not None:
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tfile:
        tfile.write(uploaded_file.read())
        video_path = tfile.name

    # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –ª–æ–≥–∏–∫–∏ ---
    rakah_counter = 0
    sajda_counter = 0
    current_pose_state = None

    st.success(f"–í–∏–¥–µ–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...")
    
    cap = cv2.VideoCapture(video_path)
    
    # --- –°–æ–∑–¥–∞–µ–º "–º–µ—Å—Ç–∞" –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –¥–ª—è –≤—ã–≤–æ–¥–∞ ---
    col1, col2 = st.columns([2, 1]) # –í–∏–¥–µ–æ –±—É–¥–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å 2/3, –º–µ—Ç—Ä–∏–∫–∏ 1/3
    with col1:
        frame_placeholder = st.empty()
    with col2:
        st.subheader("–ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞:")
        pose_placeholder = st.empty()
        sajda_placeholder = st.empty()
        rakah_placeholder = st.empty()

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break

        results = pose_model(frame, verbose=False)
        annotated_frame = results[0].plot()

        if results[0].keypoints and len(results[0].keypoints.xy) > 0:
            keypoints = results[0].keypoints.xy[0].cpu().numpy()
            flat_keypoints = keypoints.flatten()
            input_data = np.array([flat_keypoints])
            
            predicted_pose = pose_classifier.predict(input_data)[0]
            
            # --- –õ–æ–≥–∏–∫–∞ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∞ (State Machine) ---
            if predicted_pose != current_pose_state:
                if predicted_pose == 'prostrating':
                    sajda_counter += 1
                    if sajda_counter == 2:
                        rakah_counter += 1
                        sajda_counter = 0
                current_pose_state = predicted_pose

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤–∏–¥–µ–æ–∫–∞–¥—Ä
        frame_placeholder.image(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB), channels="RGB")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        pose_placeholder.metric("–¢–µ–∫—É—â–∞—è –ø–æ–∑–∞", current_pose_state if current_pose_state else "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ...")
        sajda_placeholder.metric("–°—á–µ—Ç—á–∏–∫ —Å–∞–¥–∂–¥–∞", sajda_counter)
        rakah_placeholder.metric("–°—á–µ—Ç—á–∏–∫ —Ä–∞–∫–∞–∞—Ç–æ–≤", rakah_counter)
        
    st.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    cap.release()